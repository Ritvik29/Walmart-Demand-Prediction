{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from fbprophet import Prophet\n","from tqdm import tqdm, tnrange\n","from multiprocessing import Pool, cpu_count\n","import functools\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def reduce_mem_usage(df):\n","    \"\"\" iterate through all the columns of a dataframe and modify the data type\n","        to reduce memory usage.        \n","    \"\"\"\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","    \n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        \n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","        else:\n","            df[col] = df[col].astype('category')\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## LOAD DATA"]},{"metadata":{"trusted":true},"cell_type":"code","source":["calendar_data = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n","sales_data =  pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n","price_data = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n","evaluation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n","\n","submission0 = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n","sales_data.shape"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["#sales_data = evaluation\n","status = 'validation'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## DATA TRANSFORMATION"]},{"metadata":{"trusted":true},"cell_type":"code","source":["price_data['idx'] = range(price_data.shape[0])\n","price_data_max = price_data.groupby(['item_id']).agg({'sell_price':'max'})\n","price_data_max.rename({'sell_price':'sell_price_max'},inplace = True, axis = 1)\n","price_data_max.reset_index(inplace=True)\n","price_data = price_data.merge(price_data_max,on = ['item_id'], how = 'left')\n","#price_data['discount'] =100* (price_data['sell_price_max'] - price_data['sell_price'])/price_data['sell_price_max']\n","price_data['log_sell_price'] = np.log(price_data['sell_price'])\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["price_data['id']  = price_data['item_id'] + pd.Series(np.repeat('_',price_data.shape[0])) +price_data['store_id'] + pd.Series(np.repeat('_'+str(status),price_data.shape[0]))\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["price_data['id']  = price_data['item_id'] + pd.Series(np.repeat('_',price_data.shape[0])) +price_data['store_id'] + pd.Series(np.repeat('_'+str(status),price_data.shape[0]))\n","#price_data = price_data[['id','wm_yr_wk','discount']]\n","price_data = price_data[['id','wm_yr_wk','log_sell_price']]\n","price_data = price_data.merge(calendar_data[['wm_yr_wk','d']], how = 'left', on = 'wm_yr_wk')\n","price_data = reduce_mem_usage(price_data)\n","sales_data = reduce_mem_usage(sales_data)\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["price_data['did'] = price_data['d'].apply(lambda x:x.split('_')[1])\n","price_data['did'] = price_data['did'].astype('int32')\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["common_cols = 6\n","end_time = sales_data.shape[1] - common_cols\n","start_time = end_time -2*365\n","start_idx = start_time + 5\n","start_idx\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## FEATURE CREATION"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def extract_id_info(id1):\n","    id_info= id1.split('_')\n","    state = id_info[3]\n","    category = id_info[0]\n","    return state,category\n","\n","\n","def select_snaps(df,id1):\n","    state, category = extract_id_info(id1)\n","    snap_days_CA = df[df['snap_CA']==1]['date'].unique()\n","    snap_days_TX = df[df['snap_TX']==1]['date'].unique()\n","    snap_days_WI = df[df['snap_TX']==1]['date'].unique()\n","    if state =='CA':\n","        return snap_days_CA\n","    elif state == 'TX':\n","        return snap_days_TX\n","    else:\n","        return snap_days_WI"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def get_holidays(id1):\n","\n","    Hol1_rel = calendar_data[calendar_data['event_type_1']=='Religious']['date'].unique()\n","    Hol1_nat = calendar_data[calendar_data['event_type_1']=='National']['date'].unique()\n","    Hol1_cul = calendar_data[calendar_data['event_type_1']=='Cultural']['date'].unique()\n","    Hol1_Sp = calendar_data[calendar_data['event_type_1']=='Sporting']['date'].unique()\n","\n","    #----------------------------\n","    Hol2_rel = calendar_data[calendar_data['event_type_2']=='Religious']['date'].unique()\n","    Hol2_cul = calendar_data[calendar_data['event_type_2']=='Cultural']['date'].unique()    \n","    \n","    \n","    snap_days1 = pd.DataFrame({\n","      'holiday': 'snaps',\n","      'ds': pd.to_datetime(select_snaps(calendar_data, id1)),\n","      'lower_window': 0,\n","      'upper_window': 0,\n","    })\n","\n","    \n","    holiday1_rel = pd.DataFrame({\n","      'holiday': 'holiday_religious',\n","      'ds': pd.to_datetime(Hol1_rel),\n","      'lower_window': -1,\n","      'upper_window': 1,\n","    })\n","\n","\n","\n","    holiday1_cul = pd.DataFrame({\n","      'holiday': 'holiday_cultural',\n","      'ds': pd.to_datetime(Hol1_cul),\n","      'lower_window': -1,\n","      'upper_window': 1,\n","    })\n","\n","    holiday1_nat = pd.DataFrame({\n","      'holiday': 'holiday_national',\n","      'ds': pd.to_datetime(Hol1_nat),\n","      'lower_window': -1,\n","      'upper_window': 1,\n","    })\n","\n","\n","    holiday2_cul = pd.DataFrame({\n","      'holiday': 'holiday_religious',\n","      'ds': pd.to_datetime(Hol2_cul),\n","      'lower_window': -1,\n","      'upper_window': 1,\n","    })\n","\n","\n","    holiday2_rel = pd.DataFrame({\n","      'holiday': 'holiday_religious',\n","      'ds': pd.to_datetime(Hol2_rel),\n","      'lower_window': -1,\n","      'upper_window': 1,\n","    })\n","    \n","    \n","    holidays =  pd.concat((snap_days1,holiday1_rel,holiday1_cul,holiday1_nat,holiday2_cul,holiday2_rel))\n","    return holidays\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## TRAIN MODEL"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def run_prophet(id1,data):\n","    holidays = get_holidays(id1)\n","    model = Prophet(uncertainty_samples=False,\n","                    holidays=holidays,\n","                    weekly_seasonality = True,\n","                    yearly_seasonality= True,\n","                    changepoint_prior_scale = 0.5\n","                   )\n","    \n","    model.add_seasonality(name='monthly', period=30.5, fourier_order=2)\n","    model.add_regressor('log_sell_price')\n","    try:\n","        model.fit(data)\n","        future = model.make_future_dataframe(periods=28, include_history=False)\n","        future['log_sell_price'] = np.repeat(data['log_sell_price'].iloc[-1],28)\n","        forecast2 = model.predict(future)\n","        submission = make_validation_file(id1,forecast2)\n","        return submission\n","    \n","    except:\n","        print('Failed-**************',id1)\n","        COLS = submission0.columns[0:]\n","        dd = np.hstack([np.array(id1),np.ones(28)]).reshape(1,29)\n","        submission = pd.DataFrame(dd,columns = COLS)\n","        return submission\n","    \n","    \n","\n","\n","F_cols = np.array(['F'+str(i) for i in range(1,29)])\n","\n","def make_validation_file(id1,forecast2):\n","    item_id = id1\n","    submission = pd.DataFrame(columns=F_cols)\n","    submission.insert(0,'id',item_id)\n","    forecast2['yhat'] = np.where(forecast2['yhat']<0,0,forecast2['yhat'])\n","    forecast2.rename({'yhat':'y','ds':'ds'},inplace=True,axis = 1)\n","    forecast2 = forecast2[['ds','y']].T\n","    submission.loc[1,'id'] =item_id\n","    submission[F_cols] = forecast2.loc['y',:].values[-28:]\n","    #col_order = np.insert(F_cols,0,'id')\n","    #sub_val = submission[col_order]\n","    return submission"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["\n","\n","\n","def fill_missing_did(price_series):\n","    did_a1 = price_series['did'].unique()\n","    did_range = range(start_time, end_time+1)\n","    missing_dids = [i for i in did_range if i not in did_a1]\n","    len_missing = len(missing_dids)\n","    #mode_discount = price_series['discount'].mode()[0]\n","    mode_sell_price = price_series['log_sell_price'].mode()[0]\n","    missing_t = pd.DataFrame()\n","\n","    missing_t['id'] = np.repeat(id1,len_missing)\n","    #missing_t['discount'] = np.repeat(0.0,len_missing)\n","    missing_t['log_sell_price'] = np.repeat(0.0,len_missing)\n","    missing_t['did'] = np.array(missing_dids)\n","    price_series = pd.concat([price_series,missing_t])\n","    price_series = price_series.sort_values(['did'],ascending = True)\n","    return price_series\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## RUN MODEL "]},{"metadata":{"trusted":true},"cell_type":"code","source":["\n","data_m =[]\n","test = pd.DataFrame()\n","id_lst =[]\n","dids = list(range(start_time,end_time+1))\n","counter = 0\n","price_data = price_data[['id','log_sell_price','did']]\n","for i in tnrange(sales_data.shape[0]):\n","    id1 = sales_data.iloc[i,0]\n","    id_lst.append(id1)\n","    #print(id1)\n","    data_series = sales_data.iloc[i,start_idx:]\n","    price_series = price_data[(price_data['id']==id1)]\n","    #-----------------------------------------------------------------------------\n","    price_series = price_series[(price_series['did']>=start_time) & (price_series['did']<=end_time)]\n","    price_series = fill_missing_did(price_series)\n","    \n","    #price_series = price_series[['discount']]\n","    price_series = price_series[['log_sell_price']]\n","    price_series.index = calendar_data['date'][start_idx:start_idx+len(data_series)]\n","    data_series.index = calendar_data['date'][start_idx:start_idx+len(data_series)]\n","    data_series =  pd.DataFrame(data_series)\n","    data_series = data_series.reset_index()\n","    price_series = price_series.reset_index()\n","    price_series.rename({'date':'ds'},inplace = True, axis = 1)\n","    data_series.columns = ['ds', 'y']\n","    data_series = data_series.merge(price_series, how = 'left', on = 'ds')\n","    data_series = data_series[['ds','log_sell_price','y']]\n","    #data_series = data_series[['ds','y']]\n","    data_m.append(data_series)\n","    counter +=1\n","    if id1=='FOODS_3_068_WI_2_validation':\n","        test = data_series\n","    \n","comb_lst = [(id_lst[counter],data_m[counter]) for counter in range(0,len(id_lst))]\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["#run_prophet(comb_lst[-1][0],comb_lst[-1][1])\n","#comb_lst[-100:]"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["#with open('comb_lst.npy', 'wb') as f:\n","#    np.save(f, np.array(comb_lst))\n","#import numpy as np\n","#comb_lst = np.load('../input/comb-lst/comb_lst.npy', allow_pickle = True)\n","#len(comb_lst)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["from joblib import Parallel, delayed\n","import time\n","start = time.time()\n","submission = Parallel(n_jobs=4, backend=\"multiprocessing\")(delayed(run_prophet)(comb_lst[i][0],comb_lst[i][1]) for i in range(len(comb_lst)))\n","submission = pd.concat(submission,axis =0)\n","end = time.time()\n","elapsed_time = end-start\n","time_taken = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n","print('time',time_taken)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["submission.to_csv('submission_validation_Log_price_July9_1.csv',index = False)\n","submission.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}